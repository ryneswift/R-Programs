---
title: "PGA Tour Analysis"
output: html_document
date: "2024-10-02"
---


Source for the dataset: https://www.kaggle.com/datasets/jmpark746/pga-tour-data-2010-2018?resource=download


```{r}
library(dplyr)
library(randomForest)
library(caret)
library(groupdata2)
library(dplyr)


pga_tour = read.csv("C:/Users/ryne0/Downloads/pgaTourData.csv")
pga_tour

#Descriptions of the columns of the data, copied from ChatGPT:


#SG: OTT: Measures performance on tee shots.
#SG: APP: Assesses performance on approach shots.
#SG: APR stands for Strokes Gained: Approach-the-Green
#SG: Around-the-Green (SG: ARG)

#Wins is tournament wins for the year
#gir is green in regulation percentage
#Top 10 is presumably the number of top 10 finishes for the year
```



Cleaning the data
```{r}
#Loading in our data, we see that Money is a character column. The following function changes it to int
#The function also rescales Money by dividing it by 10000, since the Money column can go up to the millions. Rescaling will make plots more readable
money_to_int = function(pga_dataset){
  
  pga_dataset$Money[pga_dataset$Money == ""] = "0"
  
  pga_dataset$Money = gsub("\\$", "", pga_dataset$Money)
  pga_dataset$Money = as.numeric(gsub(",", "", pga_dataset$Money))/10000
  
  return(pga_dataset)
}


points_to_int = function(pga_dataset){
  pga_dataset$Points[pga_dataset$Points == ""] = "0"
  pga_dataset$Points = as.numeric(gsub(",", "", pga_dataset$Points))
  
  return(pga_dataset)
}

pga_tour = money_to_int(pga_tour)
pga_tour = points_to_int(pga_tour)


#We can safely assume that when NA is present, that these two columns are blank
pga_tour$Wins[is.na(pga_tour$Wins)] = 0
pga_tour$Top.10[is.na(pga_tour$Top.10)] = 0




pga_tour_na_rows <- pga_tour[!complete.cases(pga_tour), ]
pga_tour_na_rows


#The output shows that several columns still contain NA values among the remaining rows. Given the nature of the missing information in these columns, it is impossible to infer the missing values for rows with NA entries based on the columns that remain NA. Since dropping the data would result in losing around 27% of the dataset, and there are too many NA values, we can remove the remaining rows containing NA values.


#Percentage of data lost:
(nrow(pga_tour_na_rows) / nrow(pga_tour))*100



pga_tour_cleaned = subset(pga_tour, complete.cases(pga_tour))
pga_tour_cleaned
```



We want to predict for how much money a player has won
```{r}
#Plot of money vs other variables thought to influence annual total prize winnings.
#Plot the data to examine possibe relationship between the variables.
plot_of_money_vs = function(pga_dataset, year){
  
  
  yearly_dataset = subset(pga_dataset, pga_dataset$Year == year)
  
  
  green_ir = yearly_dataset$gir
  money = yearly_dataset$Money
  plot(green_ir, money , pch = 20, xlab = "GIR (Greens in Regulation) Percentage", ylab = "Money Earned (Ten Thousands)",  main = paste("Money vs GIR in", year))
  
  
  
  avg_distance = yearly_dataset$Avg.Distance
  plot(avg_distance, money , pch = 20, xlab = "Average Distance", ylab = "Money Earned (Ten Thousands)",  main = paste("Money vs Average Distance in", year), col="blue")
  
  
  avg_putts = yearly_dataset$Average.Putts
  plot(avg_putts, money , pch = 20, xlab = "Average Number of Putts", ylab = "Money Earned (Ten Thousands)",  main = paste("Money vs Average Putts in", year), col="red")
}

plot_of_money_vs(pga_tour_cleaned, 2011)
```




Can predict for how many top 10 wins a player has for the year. make it into a binomial or trinomial category



Preparing the dataset for machine learning 
```{r}
#Sorting the dataset
pga_tour_cleaned = pga_tour_cleaned[order(pga_tour_cleaned$Year, pga_tour_cleaned$Player.Name), ]




pga_tour_money_per_year = pga_tour_cleaned %>%
  group_by(Year) %>%
  summarise(money_per_year = sum(Money, na.rm = TRUE))
pga_tour_money_per_year


#From the table we can see that total prize winnings increase over time. We will convert year into a trinomial category to address this lurking variable. The categories will go from 2010-2013, 2014-2016, and 2017-2018


pga_tour_cleaned$Year[pga_tour_cleaned$Year<2014] = 0
pga_tour_cleaned$Year[pga_tour_cleaned$Year>2013 & pga_tour_cleaned$Year<2017] = 1
pga_tour_cleaned$Year[pga_tour_cleaned$Year>2016] = 2



#Separates the data into 80% training and 20% learning, and groups players together by name such that no player can be used for both training and testing 
pga_tour_split = partition(pga_tour_cleaned, p = 0.8, cat_col = "Player.Name")
pga_tour_train = pga_tour_split[[1]]
pga_tour_test = pga_tour_split[[2]]



#From this histogram, it can be observed that roughly half of the players in the dataset did not finish in the top 10 for a given year. Because of this, conversion of top 10 into a binomial category is optional, but will not be performed to avoid loss of information.
hist(pga_tour_cleaned$Top.10)
```




rf_model_tuning_cv performs cross-validation on random forest
```{r}
rf_model_tuning_cv = function(train_dataset, ntrees, predictor_columns, k_value, response_column) {
  
  #Create k folds
  folds <- createFolds(train_dataset[[response_column]], k = k_value, list = TRUE)
  
  best_model_error = Inf
  
  for (i in 1:length(ntrees)) {
    model_errors = c()
    
    
    #Cross-validation loop
    for (j in 1:k_value) {
      
      #Split the data into training and validation
      train_fold = train_dataset[-folds[[j]], ]
      validation_fold = train_dataset[folds[[j]], ]
      
      
      rf_model = randomForest(y = train_fold[[response_column]], x = train_fold[,predictor_columns], ntree = ntrees[i])
      
      
      #Predict on the validation fold
      predictions = predict(rf_model, validation_fold[, predictor_columns])
      
      
      model_error = mean((predictions - validation_fold[[response_column]])^2)
      model_errors = c(model_errors, model_error)
    }
    
    
    #Calculate the average cross-validation error
    final_model_error = mean(model_errors)
    
    if (final_model_error < best_model_error) {
      best_model_error = final_model_error
      best_model = rf_model
    }
  }
  
  return(best_model)
}
```




Running the model
```{r}
set.seed(300)
rf_pga_model = rf_model_tuning_cv(pga_tour_train, c(125:150), c(3:17), 5, "Money")


#Getting the error of the model
tail(rf_pga_model$mse, 1)
tail(rf_pga_model$rsq, 1)


#Traditionally, a player's average shooting distance as well as performance on tee and approach shots

#We will remove any variable which has its IncNodePurity score being less than 1,000,000. We will rerun random forest to determine if the importance of our predictors has changed, and to see if our model has improved by examining its r squared value and mean squared error value.



rf_pga_model$importance
```



Rerunning the model with less predictors
```{r}
rf_pga_model_new = rf_model_tuning_cv(pga_tour_train, c(125:150), c(9:12, 14), 5, "Money")

#We observe that the mean squared error (MSE) is higher and the r squared value is lower, implying that this model is worse than the previous one. But the previous model's values may indicate that overfitting occurred. To determine which is the best model, we shall run them on our testing dataset. 
tail(rf_pga_model_new$mse, 1)
tail(rf_pga_model_new$rsq, 1)


rf_pga_model_new$importance
```




Testing the models
```{r}
set.seed(300)
rf_model_predictions = predict(rf_pga_model, pga_tour_test[,3:17])
rf_model_predictions[1:20]
pga_tour_test$Money[1:20]


rf_model_new_predictions = predict(rf_pga_model_new, pga_tour_test[,3:17])
rf_model_new_predictions[1:20]
pga_tour_test$Money[1:20]



#Below are plots of the actual money earned against the predicted money earned. The model with the most points closer to the line is the better one.

plot(pga_tour_test$Money, rf_model_predictions, main="Original Model: Actual vs Predicted", xlab="Actual Money", ylab="Predicted Money", col="blue")
abline(0, 1)


plot(pga_tour_test$Money, rf_model_new_predictions, main="New (Shortened) Model: Actual vs Predicted", xlab="Actual Money", ylab="Predicted Money", col="red")
abline(0, 1)


#From the plots, it is difficult to tell which is the better model. The following metrics will help us decide.



mse_model = mean((pga_tour_test$Money - rf_model_predictions)^2)
mse_new_model = mean((pga_tour_test$Money - rf_model_new_predictions)^2)
mse_model; mse_new_model




ss_total = sum((pga_tour_test$Money - mean(pga_tour_test$Money))^2)
ss_res_model = sum((pga_tour_test$Money - rf_model_predictions)^2)
ss_res_new_model = sum((pga_tour_test$Money - rf_model_new_predictions)^2)

r2_model = 1 - (ss_res_model / ss_total)
r2_new_model = 1 - (ss_res_new_model / ss_total)
r2_model; r2_new_model


#We see that the metrics indicate that the original model is the best model. But because our model has 10 less predictor variables, making it much more simple and efficient, and an r squared value being 0.023 less than the original model, we will keep the new model.
```





Creating interaction terms to improve our model
```{r}
#No interaction terms will be created with Wins because many rows for it are 0. 
#No interaction terms will be created for Top.10 either because its data is quite skewed, as evidenced by the histogram.

#Frequency table for Wins
table(pga_tour_train$Wins)


#Histogram
hist(pga_tour_train$Top.10, main="Histogram of Top 10 Placements", xlab="Number of Top 10 Results in a Year", col="yellow")



#Interaction terms will be created between the variables: Average.Score, Points, and Average.SG.Total


#Creating the interaction terms for the training and testing datasets

pga_tour_train["Score_Points_Interaction"] = pga_tour_train$Average.Score * pga_tour_train$Points
pga_tour_train["Score_SG_Interaction"] = pga_tour_train$Average.Score * pga_tour_train$Average.SG.Total

pga_tour_train["Points_SG_Interaction"] = pga_tour_train$Points * pga_tour_train$Average.SG.Total
pga_tour_train["Score_Points_SG_Interaction"] = pga_tour_train$Average.Score * pga_tour_train$Points * pga_tour_train$Average.SG.Total



pga_tour_test["Score_Points_Interaction"] = pga_tour_test$Average.Score * pga_tour_test$Points
pga_tour_test["Score_SG_Interaction"] = pga_tour_test$Average.Score * pga_tour_test$Average.SG.Total

pga_tour_test["Points_SG_Interaction"] = pga_tour_test$Points * pga_tour_test$Average.SG.Total
pga_tour_test["Score_Points_SG_Interaction"] = pga_tour_test$Average.Score * pga_tour_test$Points * pga_tour_test$Average.SG.Total






interaction_term_models = function(train_dataset, ntrees, predictor_columns, k_value, response_column, interaction_columns){
  
  #Holds the list of best models found in each combination of interaction terms.
  list_of_best_models = list()
  
  
  #Case where no interaction terms are included
  model_without_interactions = rf_model_tuning_cv(train_dataset, ntrees, predictor_columns, k_value, response_column)
  list_of_best_models = append(list_of_best_models, list(list(model = model_without_interactions, combination = predictor_columns)))
  
  
  # Get all possible combinations of interaction terms (excluding the case with no interaction terms)
  all_combinations = list()
  
  
  for (i in 1:length(interaction_columns)) 
    all_combinations = c(all_combinations, combn(interaction_columns, i, simplify = FALSE))
  
  
  for (column_combination in all_combinations) {
    
    #Combine the column combination with the predictor columns
    new_predictor_columns = c(predictor_columns, unlist(column_combination))  
  
    
    #Train and tune the model with cross-validation
    model = rf_model_tuning_cv(train_dataset, ntrees, new_predictor_columns, k_value, response_column)
    
  
    list_of_best_models = append(list_of_best_models, list(list(model = model, combination = new_predictor_columns)))
  }
  
  return(list_of_best_models)
}



#best_model_in_theory chooses the model with the highest r^2 value.
best_model_on_training_data = function(itms_list){
  
  #Saves which index the best model is in
  best_model_index = 0
  
  
  highest_rsq_value = 0

  #Iterating over each model
  for(i in seq_along(itms_list)){
    given_model = itms_list[[i]]
    
    
    final_rsq_value = tail(given_model$model$rsq, 1)

    
    if(final_rsq_value > highest_rsq_value){
      highest_rsq_value = final_rsq_value
      best_model_index = i
    }
    
  }
  #Retrieving the best model found and its predictor variables that were used
  best_model = itms_list[[best_model_index]]
  
  return(best_model)
}


best_model_on_testing_data = function(itm_list, testing_dataset){
  
  best_r2 = 0
  best_model_index = 0
  
  for (i in seq_along(itm_list)){
    model_predictions = predict(itm_list[[i]]$model, pga_tour_test)

    
    r2_model = r2_calculation(model_predictions, pga_tour_test)
    
    
    if (best_r2 < r2_model){
      best_r2 = r2_model
      best_model_index = i
    }
  }
  return(itm_list[[best_model_index]])
}


r2_calculation = function(predictions, testing_data){
  ss_res = sum((testing_data$Money - predictions)^2)


    
  ss_total = sum((testing_data$Money - mean(testing_data$Money))^2)

  r2_model = 1 - (ss_res / ss_total)
  
  return(r2_model)
}
```





Running the interaction terms
```{r}
pga_itms = interaction_term_models(pga_tour_train, c(125:150), c(9:12, 14), 5, "Money", c(19:22))


best_pga_interaction_model_theoretically = best_model_on_training_data(pga_itms)



print(paste("Predictor variables used in the theoretically best model that was found:", paste(colnames(pga_tour_train[, best_pga_interaction_model_theoretically$combination]), collapse = ", ")))



best_pga_interaction_model_practically = best_model_on_testing_data(pga_itms, pga_tour_test)

theoretical_interaction_model_predictions = predict(best_pga_interaction_model_theoretically$model, pga_tour_test)

practical_interaction_model_predictions = predict(best_pga_interaction_model_practically$model, pga_tour_test)


best_pga_interaction_model_theoretically; best_pga_interaction_model_practically


r2_calculation(theoretical_interaction_model_predictions, pga_tour_test); r2_calculation(practical_interaction_model_predictions, pga_tour_test)


#The best model theoretically and practically perform very similarly. But because the best model on our training dataset has a slightly higher r^2 value and a lower MSE value, we will use this as our chosen interaction model.


best_interaction_model = best_pga_interaction_model_theoretically
best_interaction_model_r2 = r2_calculation(theoretical_interaction_model_predictions, pga_tour_test)
```




Effect of quadratic variables on Model predictions
```{r}
#From these plots, we observe that there appears to be a curved relationship between each of the variables and Money. Because of this, we will examine if including quadratic terms, will improve our model's performance.

plot(pga_tour_train$Points, pga_tour_train$Money, col = "blue", main = "Plot of Points vs Money", xlab="Points", ylab = "Money", pch=20)
plot(pga_tour_train$Average.Score, pga_tour_train$Money, main = "Plot of Average Score vs Money", xlab="Average Score", ylab = "Money", pch=20)
plot(pga_tour_train$Average.SG.Total, pga_tour_train$Money, main = "Plot of SG Total vs Money", xlab="SG Total", ylab = "Money", col="red", pch=20)



#Given the nature of golf and the PGA tour, it is quite intuitive to assume that some predictor variables may have an exponential impact on how much money a player has earned. The following code will test this assumption, and determine if changing one or more variables .
#Once again Wins won't be converted to a polynomial feature, given that most of its observations are 0s and 1s and that is acting as a categorical variable. Top.10 will also be excluded from being converted to polynomial given that its data is more categorical than continuous.


#Frequency table showing Top.10 behaving more categorical than continuous
table(pga_tour_train$Top.10)



pga_tour_train["Score_Squared"] = pga_tour_train$Average.Score ^ 2
pga_tour_train["Points_Squared"] = pga_tour_train$Points ^ 2
pga_tour_train["SG_Total_Squared"] = pga_tour_train$Average.SG.Total ^ 2


pga_tour_test["Score_Squared"] = pga_tour_test$Average.Score ^ 2
pga_tour_test["Points_Squared"] = pga_tour_test$Points ^ 2
pga_tour_test["SG_Total_Squared"] = pga_tour_test$Average.SG.Total ^ 2



itm_quadratic = interaction_term_models(pga_tour_train, c(125:150), best_interaction_model$combination, 5, "Money", c(23:25))
best_quadratic_model_theoretically = best_model_on_training_data(itm_quadratic)
best_quadratic_model_theoretically



best_quadratic_model_practically = best_model_on_testing_data(itm_quadratic, pga_tour_test)
best_quadratic_model_practically



itm_quadratic


#We can see that the best model when applied to the training data differs from the best performing model which was applied to the testing data. But because the r^2 value of the best quadratic models is almost greater by 0.4 than the second best one, we can conclude that the best quadratic model is the best theoretical one. 


best_quadratic_model = best_quadratic_model_practically
best_quadratic_model_predictions = predict(best_quadratic_model$model, pga_tour_test)
best_quadratic_model_r2 = r2_calculation(best_quadratic_model_predictions, pga_tour_test)


#We also observe that the r^2 values of the quadratic models on both the testing and training data is actually slightly lower as compared to the best interaction model. Given this, we choose to go with our interaction model as the best model, because it will be simpler and less prone to overfitting.


#r^2 values on testing data
best_interaction_model_r2; best_quadratic_model_r2


#r^2 values on training data
tail(best_interaction_model$model$rsq, 1); tail(best_quadratic_model$model$rsq, 1)
```





